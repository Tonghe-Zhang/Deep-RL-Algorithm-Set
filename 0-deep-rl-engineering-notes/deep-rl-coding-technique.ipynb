{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grammar\n",
    "\n",
    "#### Python generate a list using one line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_gen=[x for x in range(5)]\n",
    "list_gen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Python generate a matrix using one line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
       " [0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
       " [0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
       " [0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
       " [0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
       " [0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
       " [0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
       " [0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
       " [0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
       " [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat=[ [y for y in range(10)] for x in range(10) ]\n",
    "mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 10)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat=np.array([ [y for y in range(10)] for x in range(10) ])\n",
    "mat\n",
    "mat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1,   1,   1,   1,   1,   1,   1,   1,   1,   1],\n",
       "       [  2,   2,   2,   2,   2,   2,   2,   2,   2,   2],\n",
       "       [  4,   4,   4,   4,   4,   4,   4,   4,   4,   4],\n",
       "       [  8,   8,   8,   8,   8,   8,   8,   8,   8,   8],\n",
       "       [ 16,  16,  16,  16,  16,  16,  16,  16,  16,  16],\n",
       "       [ 32,  32,  32,  32,  32,  32,  32,  32,  32,  32],\n",
       "       [ 64,  64,  64,  64,  64,  64,  64,  64,  64,  64],\n",
       "       [128, 128, 128, 128, 128, 128, 128, 128, 128, 128],\n",
       "       [256, 256, 256, 256, 256, 256, 256, 256, 256, 256],\n",
       "       [512, 512, 512, 512, 512, 512, 512, 512, 512, 512]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat=np.array([ [pow(2,x) for y in range(10)] for x in range(10) ])\n",
    "mat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Python unpack from tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3333333333333333 0 0.0 False\n",
      "0.3333333333333333 0 0.0 False\n",
      "0.3333333333333333 4 0.0 False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Softwares\\Anaconda\\lib\\site-packages\\gymnasium\\core.py:311: UserWarning: \u001b[33mWARN: env.P to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.P` for environment variables or `env.get_wrapper_attr('P')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "env = gym.make(\"FrozenLake-v1\", render_mode=\"human\")\n",
    "P=env.P\n",
    "P[0][0]\n",
    "for p,ss,r,terminal in P[0][0]:\n",
    "   print(p,ss,r,terminal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deque\n",
    "Importing deque\n",
    "First, you need to import deque from the collections module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from collections import deque\n",
    "\n",
    "dq = deque()            #  empty deque\n",
    "dq = deque([1, 2, 3])   # deque with initial elements\n",
    "\n",
    "dq.append(4)            # add to the right end; dq is now deque([1, 2, 3, 4])\n",
    "dq.appendleft(0)        # add to the left end; dq is now deque([0, 1, 2, 3, 4])\n",
    "\n",
    "dq.pop()                # removes and returns the rightmost item; dq is now deque([0, 1, 2, 3])\n",
    "dq.popleft()            # removes and returns the leftmost item; dq is now deque([1, 2, 3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Measure the shape/size of list, array, tensor\n",
    "For numpy.array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4 5 6 7 8 9]\n",
      "(10,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "some_array=np.array(np.arange(10))\n",
    "print(some_array)\n",
    "print(some_array.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For torch.tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.1203,  0.0282, -1.1639],\n",
      "        [ 0.3583,  1.4656,  0.3509]])\n",
      "torch.Size([2, 3])\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "a=torch.randn([2,3])\n",
    "print(a)\n",
    "print(a.shape)    # check the shape of the entire high-dim tensor\n",
    "print(a.size(1))  # check the length of each dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "a=[1,2,3]\n",
    "print(len(a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For tuple:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "a=(1,2,3)\n",
    "print(len(a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### torch.scatter \n",
    "Tensor.scatter_(dim, index, src, reduce=None) → Tensor\n",
    "Writes all values from the tensor src into self at the indices specified in the index tensor. For each value in src, its output index is specified by its index in src for dimension != dim and by the corresponding value in index for dimension = dim.\n",
    "\n",
    "For a 3-D tensor, self is updated as:\n",
    "\n",
    "self[index[i][j][k]][j][k] = src[i][j][k]  # if dim == 0\n",
    "self[i][index[i][j][k]][k] = src[i][j][k]  # if dim == 1\n",
    "self[i][j][index[i][j][k]] = src[i][j][k]  # if dim == 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to access a dictionary:\n",
    "a={\"k\" : \"v\", \"list\" : [1, {\"a\": \"1\", \"b\": \"2\", 3: \"c\"}]}\n",
    "So the entire thing is a dictionary.\n",
    "you access the elements by a['key_name']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v\n",
      "[1, {'a': '1', 'b': '2', 3: 'c'}]\n",
      "1\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "a={\"k\" : \"v\", \"list\" : [1, {\"a\": \"1\", \"b\": \"2\", 3: \"c\"}]}\n",
    "print(a['k'])\n",
    "print(a['list'])   # the element 'list' in this dictionary is a list, whose first entry is 1 and the second entry is another dictionary.\n",
    "print(a['list'][0])\n",
    "print(a['list'][1]['b'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set random seed for the entire Python Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import gymnasium as gym\n",
    "import os\n",
    "def set_seed_everywhere(env: gym.Env, seed=0):\n",
    "    # gym env\n",
    "    env.action_space.seed(seed)\n",
    "    # packages\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    # operating system\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Epsilon-greedy policy\n",
    "\n",
    "An epsilon greedy policy looks like\n",
    "$$\n",
    "\\pi_{\\epsilon}(a|s)=\n",
    "\\begin{cases}\n",
    "\\frac{\\epsilon}{\\mathcal{A}}, & a\\ne \\arg\\max_{a\\in\\mathcal{A}} Q(s,a) \\\\\n",
    "\\frac{\\epsilon}{\\mathcal{A}}+1-\\epsilon,  &a =\\arg\\max_{a\\in\\mathcal{A}} Q(s,a) \\\\\n",
    "\\end{cases}\n",
    "$$\n",
    "* If there are multiple argmax points, we only select the first one as the only greedy action.\n",
    "* $\\epsilon \\in (0,1)$ controls the randomness of the exploration.\n",
    "\n",
    "  When $\\epsilon==1$,  $\\pi_{\\epsilon}(\\cdot|s)=\\text{Unif}(\\mathcal{A})$ is the uniform policy.\n",
    "\n",
    "  When $\\epsilon==0$,  $\\pi_{\\epsilon}(\\cdot|s)=\\mathbb{1}\\{\\arg\\max_{a}Q(s,a)\\}$ is the greedy policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy_policy(nS, nA, Q_function, eps=0.5):\n",
    "    \"\"\"Get the epsilon greedy policy from the current Q function.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    nS, nA: defined at the beginning of the file\n",
    "    Q_function: np.array[nS][nA]\n",
    "        The current Q value for the given state and action\n",
    "    eps: float\n",
    "        The exploration factor epsilon\n",
    "    Returns\n",
    "    -------\n",
    "    policy: np.array[nS][nA]\n",
    "        An array of floats, policy[s][a] is the posibility of taking action a at state s\n",
    "\n",
    "    \"\"\"\n",
    "    policy = np.zeros((nS, nA))\n",
    "    m=nA\n",
    "    for s in range(nS):\n",
    "        for a in range(nA):\n",
    "            policy[s][a]=eps/m\n",
    "        a_star=np.argmax(Q_function[s])\n",
    "        policy[s][a_star]=eps/m+1-eps\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes you can attenuate the exploration density during training. Like you prefer high exploration in the first several episodes and then \n",
    "reduces random exploration in a linear speed and keep low exploration after some episodes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to draw a sample from a given distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_action(policy, state):\n",
    "    \"\"\"Sample action to take at state s according to the current policy\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    policy: np.array[nS][nA]\n",
    "        An array of floats, policy[s][a] is the possibility of taking action a at state s\n",
    "    state: int\n",
    "        Current state to take action\n",
    "    Returns\n",
    "    -------\n",
    "    action: int\n",
    "        The action to take at state s. Here we only return one sample at each call. \n",
    "    \"\"\"\n",
    "    feasible_actions=list(range(policy.shape[1]))\n",
    "    action_distribution=policy[state,:]\n",
    "    numberOfSamples=1\n",
    "    return np.random.choice(feasible_actions,size=numberOfSamples, p=action_distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 2 1]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# generate some random policy\n",
    "policy = np.random.rand(2,3)\n",
    "# normalize\n",
    "for i in range(2):\n",
    "    policy[0]=policy[0]/sum(policy[0])\n",
    "\n",
    "# draw N action samples from the policy at a given state, return a numpy array.s\n",
    "# configuration:\n",
    "state=0\n",
    "feasible_action_set=list(range(policy.shape[1]))\n",
    "numSamples=4\n",
    "action_distribution=policy[state,:]\n",
    "# generate samples:\n",
    "action_samples=np.random.choice(feasible_action_set, size=numSamples, p=action_distribution)\n",
    "print(action_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to generate Gaussian samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 Gaussian samples:\n",
      " [ 8.57298278 -0.45936931 -1.21277937  2.26635537  3.63566066 -1.02574652\n",
      "  3.42708702  0.37611235  3.64381331 -2.09348284]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# specify the structure of the Gaussian distribution.\n",
    "mean=0.3\n",
    "variance=10\n",
    "std=np.sqrt(variance)\n",
    "# specify number of samples drawn each time you call the normal() function\n",
    "num_samples=10\n",
    "# generate a batch of samples and put them into a numpy array.\n",
    "gaussian_sample_array=np.random.normal(loc=mean, scale=std,size=num_samples)\n",
    "print(f\"{num_samples} Gaussian samples:\\n {gaussian_sample_array}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Packages\n",
    "## env.P"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Description\n",
    "\n",
    "env.P is  a dictionary and $P[s][a]$ is a list. Each element $P[s][a][i]$​  is a tuple, where $i$ stands for the i-th neighbor(possible next state) of the current (s,a) pair.\n",
    "\n",
    "The tuple $P[s][a][i]$ contains four elements, (p,ss,r,terminal)\n",
    "\n",
    "where p is $P(s_i|s,a)$\n",
    "\n",
    "ss=s_i\n",
    "\n",
    "r=r(s,a,ss)\n",
    "\n",
    "terminal = whether ss is a terminal state.\n",
    "\n",
    "E.g. How to loop over this list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is P[0][0]: [(0.3333333333333333, 0, 0.0, False), (0.3333333333333333, 0, 0.0, False), (0.3333333333333333, 4, 0.0, False)]\n",
      "Element 1:\n",
      "0.3333333333333333 0 0.0 False\n",
      "Element 2:\n",
      "0.3333333333333333 0 0.0 False\n",
      "Element 3:\n",
      "0.3333333333333333 4 0.0 False\n"
     ]
    }
   ],
   "source": [
    "(s,a)=(0,0)\n",
    "print(f\"This is P[{s}][{a}]: {P[s][a]}\")\n",
    "k=1\n",
    "for p, ss, r, terminal in P[s][a]:\n",
    "    print(f\"Element {k}:\")\n",
    "    print(p,ss,r,terminal)\n",
    "    k+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Tips\n",
    "\n",
    "1. in policy iteration, value iteration, Q-learning, SARSA, whenever you call the value of the value function at the next state $s'$, you write in this way:  $V[s']*(1-{\\text{terminal}})$\n",
    "\n",
    "​        where $\\text{terminal}$ is the tag unpacked from some $P[s][a]$ indicating whether $s’$ is the terminal state. \n",
    "\n",
    "​\tIn gym, everything is finite-horizon, so if the s’ is a terminal state we choose to ignore its value.  Because terminal means dead end, and we should not reward a dead end. Since we are maximizing the reward, we should set the value at the dead end as zero.\n",
    "\n",
    "How to understand (1-done) or (1-terminal): the done flag literally means $s_{t}$ current state is already the terminal state, so \n",
    "there will be no future returns. Recall that in Bellman update,\n",
    "$Q(s,a)=r(s,a)+\\gamma \\mathbb{E}_{s_{t+1}}\\mathbb{E}_{a_{t+1}\\sim \\pi(\\cdot|s_{t+1})}Q(s_{t+1},a_{t+1})$\n",
    "the second part indicates future returns. So if $s_{t}$ is already in the terminal state (done==True), then we only receive \n",
    "$r(s_t,a_t)$ but there will be no $Q(s_{t+1},a_{t+1})$. So we add a multiplier to the second term:\n",
    "$$\n",
    "Q(s,a)=r(s,a)+\\gamma \\mathbb{E}_{s_{t+1}}\\mathbb{E}_{a_{t+1}\\sim \\pi(\\cdot|s_{t+1})}Q(s_{t+1},a_{t+1})*(1-done)\n",
    "$$\n",
    "\n",
    "2. The rewards in gym depends not just on the current (s,a) pairs, but also the next state $s'$. \n",
    "\n",
    "   So the Bellman equation literarily becomes \n",
    "   $$\n",
    "   Q(s,a)=\\int_{\\mathcal{S}} P(ds'|s,a)~\\left[r(s,a,s')+\\gamma~ V(s')\\cdot (1-\\text{terminal}) \\right]\n",
    "   $$\n",
    "   Remember that this is different from the theoretical convention\n",
    "\n",
    "   $Q=r+\\gamma PV$​\n",
    "\n",
    "\n",
    "\n",
    "2. The Q-learning and SARSA implementations are, in general, asynchronous,  because usually the gym environment is sampling new states in a Markovian(online, simulator-free) manner. So each step in Q-learning we only update the sampled state-action pair.\n",
    "\n",
    "   (s,a) ==> (s,a,r, s’, a’)\n",
    "\n",
    "\n",
    "\n",
    "2. Value iteration converges much slower than policy iteration, empirically and theoretically. \n",
    "\n",
    "\n",
    "\n",
    "2. Differences between Q-learning and SARSA:\n",
    "\n",
    "   In each update step, Q-learning does not care about the actul next action, but it needs the previous policy. Q-learning is off-policy. \n",
    "\n",
    "   Whiles SARSA’s update rule requires the knowledge of the next action $a'$ from the Markov sampler. SARSA is on-policy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hydra\n",
    "Official website:  https://hydra.cc/docs/intro/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replay buffer in RL\n",
    "\n",
    "#### Definition:\n",
    "You store multiple previously collected (s,a,s', r, terminates) tuples (called trajectories) into a reply buffer with some capacity $C$.\n",
    "Then you draw random trajectories from the reply buffer and feed them to the agent during training. \n",
    "So you can alleviate the need to draw new samples from the simulator or the real-environment. \n",
    "\n",
    "#### Why do we use them?\n",
    "\n",
    "* Off-Policy Training: the agent learns and improves its policy using experiences collected using a different policy, so it enables **off-policy training**. This can help **stabilize training**.\n",
    "\n",
    "* Experience Replay: A replay buffer stores past experiences (transitions of state, action, reward, next state) encountered by the agent and then randomly sample experiences from the replay buffer during training, **you break correlations in the sequence of experience updates**. reduces the agent's dependence on the most recent experiences, **prevent the agent from focusing too heavily on recent, possibly misleading, experiences.**\n",
    "\n",
    "* Efficient Utilization of Experience: \n",
    "**Reuse historic data**: reduce **sample-complexity**\n",
    "**Random sampling from the replay buffer**  allows the agent to learn from a **more diverse set of experiences** compared to **learning from consecutive experiences**. This can lead to  **better exploration** of the state-action space. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for episode in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    while not done:\n",
    "        # online interaction with the environment.\n",
    "        action = agent.select_action(state)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "        # store a bathc of experience to the reply buffer.\n",
    "        experience = (state, action, reward, next_state, done)\n",
    "        replay_buffer.add(experience)\n",
    "\n",
    "        # Sample a batch from the replay buffer and update the agent\n",
    "        batch = replay_buffer.sample(batch_size)\n",
    "        agent.update(batch)  # the agent updates the policy/value function using the bathed data from the replay buffer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-step replay buffer\n",
    "Calculating the return (cumulative reward) over N steps rather than the immediate reward returned after a single step.\n",
    "\n",
    "This approach can help in learning more efficiently by considering the future rewards and thus adjusting the agent's actions for better long-term outcomes.\n",
    "\n",
    "By considering rewards over N steps, the algorithm learns policies that are more far-sighted, potentially improving learning efficiency and stability.\n",
    "\n",
    "Like a traditional replay buffer, it stores past experiences allowing the agent to learn from a more diverse set of experiences than just the most recent ones, reducing the correlation between consecutive learning samples and improving stability.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target Network\n",
    "What is a Target Network?\n",
    "**A target network** is a copy of the original neural network model (often referred to as **the online network**) that is used to generate the Q-values of the next state in the Q-learning update rule. \n",
    "\n",
    "Unlike the online network, which is updated at every step (or every few steps) of training, the target network's weights are kept frozen for a number of steps before being updated with the weights of the online network. \n",
    "\n",
    "This means that the **target network provides a stable, albeit slightly outdated, set of Q-values** against which the online network's Q-values can be compared and updated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Update of the target network\n",
    "\n",
    "**Hard Update vs. Soft Update**\n",
    "Hard Update: \n",
    "\n",
    "In the original DQN algorithm, the target network’s weights are periodically copied from the online network’s weights every ( N ) steps. This is known as a hard update. \n",
    "\n",
    "While this approach is straightforward and ensures stability, it can introduce significant changes to the policy being evaluated, potentially causing instability or oscillation in the learning process.\n",
    "\n",
    "Soft Update: \n",
    "\n",
    "To address the potential downsides of hard updates, some variations of DQN employ a soft update mechanism. In soft update, instead of copying the weights directly every ( N ) steps, the weights of the target network are gradually updated towards the weights of the online network at every time step or at fixed intervals shorter than those of hard updates. This is where ( \\tau ) comes in.\n",
    "\n",
    "**The Role of $\\tau$**\n",
    "The parameter $\\tau$ controls the extent to which the weights of the online network influence the update of the target network’s weights during soft updates. The update rule for soft updates can be described as follows:\n",
    "\n",
    "$\\theta_{\\text{target}} = \\tau \\theta_{\\text{online}} + (1 - \\tau) \\theta_{\\text{target}}$\n",
    "\n",
    "where:\n",
    "$ \\tau$ are the weights of the target network,\n",
    "$\\theta_{\\text{online}}$ are the weights of the online network,\n",
    "$\\tau $ is a small value (e.g., 0.001) indicating how much of the online network’s weights should be “blended” into the target network’s weights.\n",
    "This formula shows that after the update, the target network’s weights become a blend of their previous values and the current values of the online network’s weights. The degree of blending is controlled by ( \\tau ):\n",
    "\n",
    "When $\\tau = 1$, it’s effectively a hard update (the target network becomes an exact copy of the online network).\n",
    "When $\\tau$  is very small, the target network changes only slightly at each step, ensuring smoother and potentially more stable learning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def soft_update(self, target, source):\n",
    "    for target_param, source_param in zip(target.parameters(), source.parameters()):\n",
    "        target_param.data.copy_((1 - self.tau) * target_param.data + self.tau * source_param.data)\n",
    "    # notice that we used in-place copy_() to update target param network right away."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PyTorch implementation of soft update:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def soft_update(self, target, source):\n",
    "    for target_param, source_param in zip(target.parameters(), source.parameters()):\n",
    "        target_param.data.copy_(self.tau * source_param.data + (1 - self.tau) * target_param.data)\n",
    "    # notice that we used in-place copy_() to update target param network right away."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PyTorch Implementation of periodically update of target nets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = torch.mean((Q - Q_target)**2 * weights)\n",
    "\n",
    "self.optimizer.zero_grad()\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "self.optimizer.step()\n",
    "\n",
    "# periodically update the target network.\n",
    "if not step % self.target_update_interval:\n",
    "    self.soft_update(self.target_net, self.q_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "**Advantages of Soft Updates**\n",
    "\n",
    "Soft updates can provide a more stable learning environment for the agent by avoiding sudden large changes in the policy that it evaluates against. By making the transition of the target network's policy smooth over training iterations, it helps maintain a consistent direction of policy improvement and potentially leads to better convergence properties.\n",
    "\n",
    "It's important to choose an appropriate $\\tau$ value - too small might make the target network’s update too slow, potentially delaying convergence, while too large can make the target network change too quickly, reducing the stability benefits of having a target network in the first place."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q target and the DQNet variants\n",
    "\n",
    "* What is Q target: the estimate of the optimal future value.  **The maximum sum of rewards achievable from a given state-action pair, under the optimal policy.**. \n",
    "\n",
    "* How to compute it:  $Q_{\\text{target}} = r + \\gamma \\max_{a'}Q(s', a')$. The Q target thus represents the expected return of taking action (a) in state (s) and following the optimal policy thereafter.\n",
    "\n",
    "* Why do we use it: **compute the loss and update the model’s weights.**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The overestimation problem of Deep Q Nets\n",
    "1. The DQN is an implementation of the Bellman equations using neural networks as function approximators in the high-dimensional spaces. The Bellman equations, being purely mathematical derivations, are correct in its essence and it will\n",
    "not lead to over-estimation. \n",
    "2. The problem of over-estimation comes not from the Bellman equations, but from the training of Deep neutral networks. So it is incurred purely due to the imperfections of function approximators. \n",
    "3. Specifically, the Bellman equations update the q value by\n",
    "   $$\n",
    "   Q(s,a)=r(s,a)+\\gamma \\cdot \\max_{a\\in \\mathcal{A}} Q(s,a)\n",
    "   $$\n",
    "   which involes an $\\max(\\cdot)$ operator.\n",
    "   However, due to the noise in data samples and numerical erros in computation, as well as function approximator error brought by the limited expressivity of neural nets, the Q functions actually computed by the algorithm could be highly inaccurate, especially during the first several iterations. \n",
    "   Then the Bellman updates could have some errors. Since Q functions are renewed by $\\max$ opertors, the Q-values updated by the algorithm tends to be over-estimated.\n",
    "   Since we pick $\\max()$ in each step of the Bellman updates, this over-estimation will propagate along the horizon. \n",
    "4. How to solve over-estimation:\n",
    "   \n",
    "   * Double Deep Q Networks (DDQN): Uses two separate value functions to decouple the action selection and policy evaluation.\n",
    "\n",
    "    **Behavior Policy Network $\\theta$** : for action selection; uses the current weights.\n",
    "    \n",
    "    **Target Policy Network $\\theta^-$**: for evaluating the action's value; uses a separate set of weights that are periodically updated with the weights of the Behavior Policy Network. when it is not updated, the weights are obsolete.\n",
    "\n",
    "    We combine the two networks to update the Q value, which will be called the Q target:\n",
    "    $$\n",
    "    Q_{\\text{target}} = r + \\gamma Q(s', \\arg\\max_{a'}Q(s', a'; \\theta)\\ ;\\ \\theta^-)\n",
    "    $$\n",
    "    this formula clearly shows that we use the behavior policy network $\\theta$to select the action\n",
    "    $$\n",
    "    a'=\\arg\\max_{a}Q(s', a; \\theta)\n",
    "    $$\n",
    "    and we use the target policy network $\\theta^-$ to evaluate the action's value:\n",
    "    $$\n",
    "    Q_{\\text{target}} = r + \\gamma Q(s', a'; \\theta^-)\n",
    "    $$\n",
    "\n",
    "    In the ideal case, the Q values for action selection and policy evaluation should be the same, meaning that $Q_{\\text{target}}$ should also be identical to $Q$. \n",
    "    To evaluate the actual difference between the two values, we define the loss function as\n",
    "    $$\n",
    "    Loss=(Q - Q_{\\text{target}})^2\n",
    "    $$\n",
    "   \n",
    "    * Deuling Q networks"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
